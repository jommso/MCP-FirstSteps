# Mix Server - Lokales MCP-System

Ein vollstÃ¤ndig lokales MCP (Model Context Protocol) System mit Server und Client, das mit Ollama und LlamaIndex arbeitet.

## Ãœberblick

Dieses Projekt demonstriert, wie man ein komplettes lokales MCP-System baut, das:
- Einen MCP-Server mit benutzerdefinierten Tools fÃ¼r CSV- und Parquet-Dateien
- Einen lokalen Client mit LlamaIndex und Ollama
- VollstÃ¤ndig lokale Verarbeitung ohne externe API-Aufrufe

## Projektstruktur

```
mix_server/
â”‚
â”œâ”€â”€ data/                 # Beispiel-CSV- und Parquet-Dateien
â”‚   â”œâ”€â”€ sample.csv
â”‚   â””â”€â”€ sample.parquet
â”‚
â”œâ”€â”€ tools/                # MCP-Tool-Definitionen
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ csv_tools.py
â”‚   â””â”€â”€ parquet_tools.py
â”‚
â”œâ”€â”€ utils/                # Wiederverwendbare Datei-Leselogik
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ file_reader.py
â”‚
â”œâ”€â”€ server.py             # MCP-Server-Instanz
â”œâ”€â”€ main.py               # Server-Einstiegspunkt
â”œâ”€â”€ client.py             # Lokaler MCP-Client
â”œâ”€â”€ generate_parquet.py   # Hilfsskript fÃ¼r Parquet-Datei
â””â”€â”€ README.md             # Diese Datei
```

## Installation und Setup

### Voraussetzungen

1. **Ollama installieren**
   ```bash
   # Gehe zu https://ollama.ai und folge den Anweisungen
   ollama pull llama3.2
   ```

2. **Python-AbhÃ¤ngigkeiten installieren**
   ```bash
   uv add "mcp[cli]" pandas pyarrow llama-index llama-index-llms-ollama llama-index-tools-mcp nest-asyncio
   ```

3. **Parquet-Datei generieren**
   ```bash
   uv run generate_parquet.py
   ```

## Verwendung

### 1. MCP-Server starten

```bash
uv run main.py
```

### 2. Ollama-Service starten (falls nicht laufend)

```bash
ollama serve
```

### 3. Client starten

```bash
uv run client.py
```

## VerfÃ¼gbare Tools

- **summarize_csv_file**: Liest und fasst CSV-Dateien zusammen
- **summarize_parquet_file**: Liest und fasst Parquet-Dateien zusammen

## Beispiel-Interaktionen

```
ðŸ’¬ Gib deine Nachricht ein: Fasse die CSV-Datei namens sample.csv zusammen
ðŸ’¬ Gib deine Nachricht ein: Vergleiche sample.csv und sample.parquet
ðŸ’¬ Gib deine Nachricht ein: Wie viele Zeilen hat sample.csv?
```

## Erweiterung

Das System ist modular aufgebaut und kann einfach erweitert werden:

1. **Neue Tools hinzufÃ¼gen**: Erstelle neue Python-Dateien im `tools/`-Ordner
2. **Neue Datenformate unterstÃ¼tzen**: Erweitere die `utils/file_reader.py`
3. **Neue FunktionalitÃ¤ten**: FÃ¼ge Datenbank-Zugriff, API-Calls oder andere Features hinzu

## Funktionsweise

1. Der MCP-Server stellt Tools Ã¼ber das Model Context Protocol zur VerfÃ¼gung
2. Der Client verbindet sich mit dem Server und ruft verfÃ¼gbare Tools ab
3. Ein lokales LLM (Ã¼ber Ollama) entscheidet, welche Tools aufgerufen werden
4. LlamaIndex orchestriert die Kommunikation zwischen LLM, Client und Server
5. Alle Daten bleiben vollstÃ¤ndig lokal auf deinem System

## Fehlerbehebung

- **Server-Verbindungsfehler**: Stelle sicher, dass der Server auf Port 8000 lÃ¤uft
- **Ollama-Probleme**: ÃœberprÃ¼fe mit `ollama list`, ob das Modell verfÃ¼gbar ist
- **Import-Fehler**: FÃ¼hre `uv sync` aus, um alle AbhÃ¤ngigkeiten zu installieren